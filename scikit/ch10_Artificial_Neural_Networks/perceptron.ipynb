{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150, 2)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)]\n",
    "y = (iris.target==0).astype(np.int)\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tales\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X,y)\n",
    "y_pred = per_clf.predict([[1,0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# teste or_exclusive\n",
    "xor = np.array([[0.,0.],[0.,1.],[1.,1.],[1.,0.]])\n",
    "y_target = np.array([0,1,0,1])\n",
    "print(xor.shape)\n",
    "print(y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tales\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perceptron n√£o consegue definir xor\n",
    "xor_clf = Perceptron(random_state=42)\n",
    "xor_clf.fit(xor,y_target)\n",
    "y_pred = xor_clf.predict([[1,0]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_hidden1 = 2\n",
    "n_vars = xor.shape[0]\n",
    "n_output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'labels' has DataType float64 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-305f1e83db0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogits1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outputs1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mxentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxentropy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#hyperparametro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m       cost, _ = gen_nn_ops._sparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m-> 1693\u001b[1;33m           precise_logits, labels, name=name)\n\u001b[0m\u001b[0;32m   1694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m_sparse_softmax_cross_entropy_with_logits\u001b[1;34m(features, labels, name)\u001b[0m\n\u001b[0;32m   2489\u001b[0m   \"\"\"\n\u001b[0;32m   2490\u001b[0m   result = _op_def_lib.apply_op(\"SparseSoftmaxCrossEntropyWithLogits\",\n\u001b[1;32m-> 2491\u001b[1;33m                                 features=features, labels=labels, name=name)\n\u001b[0m\u001b[0;32m   2492\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_SparseSoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    587\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[0;32m    588\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                                        param_name=input_name)\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[1;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[0;32m     58\u001b[0m           \u001b[1;34m\"allowed values: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[1;32m---> 60\u001b[1;33m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Value passed to parameter 'labels' has DataType float64 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "X = tf.constant (xor,name='X',dtype=tf.float64)\n",
    "y = tf.constant (y_target,name='y')\n",
    "hidden1 = tf.layers.dense (X,n_hidden1, name='hidden1',activation=tf.nn.relu)\n",
    "logits1 = tf.layers.dense (hidden1,n_output,name='outputs1')\n",
    "#loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits1)\n",
    "loss = tf.reduce_mean(xentropy,name='loss')\n",
    "#hyperparametro\n",
    "learning_rate = 0.01\n",
    "#gradiente\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "#erros\n",
    "correct = tf.nn.in_top_k(logits1, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n",
      "treinando : Tensor(\"SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\", shape=(4,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_epochs = 50001\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "        #acc_train = accuracy.eval()\n",
    "        #acc_val = accuracy.eval()\n",
    "        if epoch%5000 == 0:\n",
    "            xentropy.eval()\n",
    "            print('treinando :',xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder (tf.float32, shape=(n_vars,2),name='X')\n",
    "y = tf.placeholder (tf.int64, shape=(n_vars),name='y')# 1D tensor\n",
    "hid1 = tf.layers.dense (X,n_hidden1, name='hid1',activation=tf.nn.relu)\n",
    "logits1 = tf.layers.dense (hid1,n_output,name='outputs1')\n",
    "#loss function\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits1)\n",
    "loss = tf.reduce_mean(xentropy,name='loss')\n",
    "#hyperparametro\n",
    "learning_rate = 0.01\n",
    "#gradiente\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "#erros\n",
    "correct = tf.nn.in_top_k(logits1, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.0 Val accuracy: 0.0\n",
      "5000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "10000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "15000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "20000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "25000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "30000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "35000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "40000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "45000 Train accuracy: 1.0 Val accuracy: 1.0\n",
      "50000 Train accuracy: 1.0 Val accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_epochs = 50001\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: xor, y: y_target})\n",
    "        acc_train = accuracy.eval(feed_dict={X: xor, y: y_target})\n",
    "        acc_val = accuracy.eval(feed_dict={X: xor, y: y_target,\n",
    "                                            X: xor, y: y_target})\n",
    "        if epoch%5000 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando uma deep neural network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construction Phase\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder (tf.float32, shape=(None,n_inputs),name='X') #2D tensor - nao sabe qtas instancias o lote de treinamento ter√°\n",
    "y = tf.placeholder (tf.int64, shape=(None),name='y')# 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fun√ß√£o para criar uma camada de neuronios\n",
    "def neuron_layer(X,n_neurons,name,activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1]) # pegar a quantidade de entrada da matriz X. segunda coordenada pq a 1 √© o numero de instancias\n",
    "        stddev = 2/np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs,n_neurons),stddev = stddev)\n",
    "        W = tf.Variable(init,name='kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]),name='bias')\n",
    "        Z = tf.matmul(X,W)+b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# criar a DNN manualmente\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer (X,n_hidden1, name='hidden1',activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer (hidden1,n_hidden2,name='hidden2',activation=tf.nn.relu)\n",
    "    logits = neuron_layer (hidden2,n_output,name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# criar DNN automaticamente sem a fun√ß√£o neuron_layer criada anteriormente\n",
    "#with tf.name_scope('dnn'):\n",
    "#    hidden1 = tf.layers.dense (X,n_hidden1, name='hidden1',activation=tf.nn.relu)\n",
    "#    hidden2 = tf.layers.dense (hidden1,n_hidden2,name='hidden2',activation=tf.nn.relu)\n",
    "#    logits = tf.layers.dense (hidden2,n_output,name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Val accuracy: 0.9154\n",
      "1 Train accuracy: 0.96 Val accuracy: 0.935\n",
      "2 Train accuracy: 0.98 Val accuracy: 0.9404\n",
      "3 Train accuracy: 0.96 Val accuracy: 0.9498\n",
      "4 Train accuracy: 0.98 Val accuracy: 0.9538\n",
      "5 Train accuracy: 0.94 Val accuracy: 0.9564\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.9586\n",
      "7 Train accuracy: 1.0 Val accuracy: 0.9626\n",
      "8 Train accuracy: 0.98 Val accuracy: 0.963\n",
      "9 Train accuracy: 0.98 Val accuracy: 0.9658\n",
      "10 Train accuracy: 0.98 Val accuracy: 0.9676\n",
      "11 Train accuracy: 0.98 Val accuracy: 0.9682\n",
      "12 Train accuracy: 0.98 Val accuracy: 0.9694\n",
      "13 Train accuracy: 1.0 Val accuracy: 0.9698\n",
      "14 Train accuracy: 0.98 Val accuracy: 0.9698\n",
      "15 Train accuracy: 0.98 Val accuracy: 0.9722\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.9736\n",
      "17 Train accuracy: 1.0 Val accuracy: 0.9726\n",
      "18 Train accuracy: 0.98 Val accuracy: 0.9738\n",
      "19 Train accuracy: 1.0 Val accuracy: 0.9746\n",
      "20 Train accuracy: 1.0 Val accuracy: 0.9762\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.9768\n",
      "22 Train accuracy: 0.98 Val accuracy: 0.9762\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.9766\n",
      "24 Train accuracy: 0.98 Val accuracy: 0.9774\n",
      "25 Train accuracy: 1.0 Val accuracy: 0.9754\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.977\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.977\n",
      "28 Train accuracy: 0.98 Val accuracy: 0.9772\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9774\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.9786\n",
      "31 Train accuracy: 0.98 Val accuracy: 0.979\n",
      "32 Train accuracy: 0.98 Val accuracy: 0.9782\n",
      "33 Train accuracy: 0.96 Val accuracy: 0.9782\n",
      "34 Train accuracy: 1.0 Val accuracy: 0.9794\n",
      "35 Train accuracy: 0.98 Val accuracy: 0.9772\n",
      "36 Train accuracy: 1.0 Val accuracy: 0.9788\n",
      "37 Train accuracy: 1.0 Val accuracy: 0.9798\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.979\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                            y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "Actual classes:    [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
